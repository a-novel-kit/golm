package models

type ChatCompletionResponse struct {
	// A unique identifier for the chat completion.
	ID string `json:"id"`
	// The Unix timestamp (in seconds) of when the chat completion was created.
	Created int `json:"created"`
	// The model used for the chat completion.
	Model Model `json:"model"`
	// The object type, which is always `chat.completion`.
	Object string `json:"object"`
	// This fingerprint represents the backend configuration that the model runs with.
	//
	// Can be used in conjunction with the `seed` request parameter to understand when
	// backend changes have been made that might impact determinism.
	SystemFingerprint string `json:"system_fingerprint,omitempty"`
	// A list of chat completion choices. Can be more than one if `n` is greater than 1.
	Choices []ChatCompletionChoice `json:"choices"`
	// Usage statistics for the completion request.
	Usage ChatCompletionUsage `json:"usage"`
}

type ChatCompletionChunkResponse struct {
	// A unique identifier for the chat completion.
	ID string `json:"id"`
	// A list of chat completion choices. Can be more than one if `n` is greater than 1.
	Choices []ChatCompletionChunkChoice `json:"choices"`
	// The Unix timestamp (in seconds) of when the chat completion was created.
	Created int `json:"created"`
	// The model used for the chat completion.
	Model Model `json:"model"`
	// The object type, which is always `chat.completion.chunk`.
	Object string `json:"object"`
	// This fingerprint represents the backend configuration that the model runs with.
	//
	// Can be used in conjunction with the `seed` request parameter to understand when
	// backend changes have been made that might impact determinism.
	SystemFingerprint string `json:"system_fingerprint,omitempty"`

	XGroq *ChatCompletionXGroq `json:"x_groq,omitempty"`
}

type ChatCompletionXGroq struct {
	// A groq request ID which can be used by to refer to a specific request to groq support Only sent with the first
	// chunk.
	ID string `json:"id"`
	// An error string indicating why a stream was stopped early.
	Error string `json:"error"`
	// Usage information for the stream. Only sent in the final chunk.
	Usage ChatCompletionUsage `json:"usage"`
}

type ChatCompletionUsage struct {
	// Number of tokens in the generated completion.
	CompletionTokens int `json:"completion_tokens"`
	// Number of tokens in the prompt.
	PromptTokens int `json:"prompt_tokens"`
	// Total number of tokens used in the request (prompt + completion).
	TotalTokens int `json:"total_tokens"`
	// Time spent generating tokens
	GenerationTime float64 `json:"generation_time"`
	// Time spent processing input tokens
	ProcessingTime float64 `json:"processing_time"`
	// Time the requests was spent queued
	QueuedTime float64 `json:"queued_time"`
	// Completion time and prompt time combined.
	TotalTime float64 `json:"total_time"`
}

type ChatCompletionChoice struct {
	// The reason the model stopped generating tokens. This will be `stop` if the model
	// hit a natural stop point or a provided stop sequence, `length` if the maximum
	// number of tokens specified in the request was reached, `tool_calls` if the model
	// called a tool, or `function_call` (deprecated) if the model called a function.
	FinishReason ChatCompletionChoiceFinishReason `json:"finish_reason"`
	// The index of the choice in the list of choices.
	Index int `json:"index"`
	// Log probability information for the choice.
	Logprobs ChatCompletionChoiceLogProbs `json:"logprobs"`
	// A chat completion message generated by the model.
	Message ChatCompletionMessage `json:"message"`
}

type ChatCompletionChunkChoice struct {
	// The reason the model stopped generating tokens. This will be `stop` if the model
	// hit a natural stop point or a provided stop sequence, `length` if the maximum
	// number of tokens specified in the request was reached, `tool_calls` if the model
	// called a tool, or `function_call` (deprecated) if the model called a function.
	FinishReason *ChatCompletionChoiceFinishReason `json:"finish_reason"`
	// Log probability information for the choice.
	Logprobs *ChatCompletionChoiceLogProbs `json:"logprobs"`
	// The index of the choice in the list of choices.
	Index int `json:"index"`
	// A chat completion message generated by the model.
	Delta ChatCompletionChunkDelta `json:"delta"`
}

type ChatCompletionChoiceFinishReason string

const (
	ChatCompletionChoiceFinishReasonStop         ChatCompletionChoiceFinishReason = "stop"
	ChatCompletionChoiceFinishReasonLength       ChatCompletionChoiceFinishReason = "length"
	ChatCompletionChoiceFinishReasonToolCalls    ChatCompletionChoiceFinishReason = "tool_calls"
	ChatCompletionChoiceFinishReasonFunctionCall ChatCompletionChoiceFinishReason = "function_call"
)

type ChatCompletionChoiceLogProbs struct {
	// A list of message content tokens with log probability information.
	Content []ChatCompletionTokenLogprob `json:"content,omitempty"`
}

type ChatCompletionTokenLogprob struct {
	// The token.
	Token string `json:"token"`
	// A list of integers representing the UTF-8 bytes representation of the token.
	// Useful in instances where characters are represented by multiple tokens and
	// their byte representations must be combined to generate the correct text
	// representation. Can be `null` if there is no bytes representation for the token.
	Bytes []byte `json:"bytes"`
	// The log probability of this token, if it is within the top 20 most likely
	// tokens. Otherwise, the value `-9999.0` is used to signify that the token is very
	// unlikely.
	LogProb float64 `json:"logprob"`
	// List of the most likely tokens and their log probability, at this token
	// position. In rare cases, there may be fewer than the number of requested
	// `top_logprobs` returned.
	TopLogprobs []ChatCompletionTokenTopLogprob `json:"top_logprobs"`
}

type ChatCompletionTokenTopLogprob struct {
	// The token.
	Token string `json:"token"`
	// A list of integers representing the UTF-8 bytes representation of the token.
	// Useful in instances where characters are represented by multiple tokens and
	// their byte representations must be combined to generate the correct text
	// representation. Can be `null` if there is no bytes representation for the token.
	Bytes []byte `json:"bytes"`
	// The log probability of this token, if it is within the top 20 most likely
	// tokens. Otherwise, the value `-9999.0` is used to signify that the token is very
	// unlikely.
	LogProb float64 `json:"logprob"`
}

type ChatCompletionMessage struct {
	// The contents of the message.
	Content string `json:"content,omitempty"`
	// The role of the author of this message.
	Role MessageRole `json:"role,omitempty"`
	// The model's reasoning for a response. Only available for reasoning models when requests parameter
	// ChatCompletionRequest.ReasoningFormat has value `parsed.
	Reasoning string `json:"reasoning,omitempty"`
	// The tool calls generated by the model, such as function calls.
	ToolCalls []ChatCompletionToolCall `json:"tool_calls,omitempty"`
}

type ChatCompletionChunkDelta struct {
	// The contents of the message.
	Content string `json:"content,omitempty"`
}

type ChatCompletionToolCall struct {
	// The ID of the tool call.
	ID string `json:"id"`
	// The type of the tool. Currently, only ToolTypeFunction is supported.
	Type ToolType `json:"type,omitempty"`
	// The function that the model called.
	Function *ToolCallFunction `json:"function,omitempty"`
}
